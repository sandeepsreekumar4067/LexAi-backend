[
    {
        "label": "PyPDFLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "OllamaLLM",
        "importPath": "langchain_ollama.llms",
        "description": "langchain_ollama.llms",
        "isExtraImport": true,
        "detail": "langchain_ollama.llms",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "handle_query",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def handle_query(query, context=\"\"):\n    # If document search is triggered\n    if \"section\" in query.lower() or \"pdf\" in query.lower():\n        results = vector_store.similarity_search(query)\n        if results:\n            context = results[0].page_content  # Add the first result to the context\n    # Construct the full prompt with context and query\n    full_prompt = chat_prompt.format(query=query, context=context)\n    return llm.invoke(full_prompt)\n# Example usage",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "llm = OllamaLLM(model=\"llama3.1\")\nembedding_model = OllamaEmbeddings(model=\"llama3.1\")\n# uploading the pdf\nprint(\"uploaded the pdf\")\npdf_reader = PyPDFLoader(\"../Backend/assets/ipc.pdf\")\ndocuments = pdf_reader.load_and_split()\n# pages = 'hello'\n# print(pages)\nchat_prompt = ChatPromptTemplate.from_messages([\n    \"\"\"",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "embedding_model",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "embedding_model = OllamaEmbeddings(model=\"llama3.1\")\n# uploading the pdf\nprint(\"uploaded the pdf\")\npdf_reader = PyPDFLoader(\"../Backend/assets/ipc.pdf\")\ndocuments = pdf_reader.load_and_split()\n# pages = 'hello'\n# print(pages)\nchat_prompt = ChatPromptTemplate.from_messages([\n    \"\"\"\n    <s>[INST] You are an AI legal assistant, skilled in Indian law. Provide accurate legal advice or document lookup based on the query. If you do not have sufficient information to answer, please advise on the next steps or suggest seeking professional legal help. [/INST]</s>",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "pdf_reader",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "pdf_reader = PyPDFLoader(\"../Backend/assets/ipc.pdf\")\ndocuments = pdf_reader.load_and_split()\n# pages = 'hello'\n# print(pages)\nchat_prompt = ChatPromptTemplate.from_messages([\n    \"\"\"\n    <s>[INST] You are an AI legal assistant, skilled in Indian law. Provide accurate legal advice or document lookup based on the query. If you do not have sufficient information to answer, please advise on the next steps or suggest seeking professional legal help. [/INST]</s>\n    [INST] Question: {query}\n           Context: {context}\n           Response:",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "documents = pdf_reader.load_and_split()\n# pages = 'hello'\n# print(pages)\nchat_prompt = ChatPromptTemplate.from_messages([\n    \"\"\"\n    <s>[INST] You are an AI legal assistant, skilled in Indian law. Provide accurate legal advice or document lookup based on the query. If you do not have sufficient information to answer, please advise on the next steps or suggest seeking professional legal help. [/INST]</s>\n    [INST] Question: {query}\n           Context: {context}\n           Response:\n    [/INST]",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "chat_prompt",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "chat_prompt = ChatPromptTemplate.from_messages([\n    \"\"\"\n    <s>[INST] You are an AI legal assistant, skilled in Indian law. Provide accurate legal advice or document lookup based on the query. If you do not have sufficient information to answer, please advise on the next steps or suggest seeking professional legal help. [/INST]</s>\n    [INST] Question: {query}\n           Context: {context}\n           Response:\n    [/INST]\n\"\"\"\n])\nprint(\"creating the text splitter\")",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1024, chunk_overlap=200, length_function=len\n)\n# creating the chunks\nprint(\"creating the chunks\")\nchunks = text_splitter.split_documents(documents)\nprint(\"creating chroma db\")\n# creating the vector store\n# vector_store = Chroma.from_documents(\n#     chunks, embedding_model, persist_directory=\"db\"",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "chunks = text_splitter.split_documents(documents)\nprint(\"creating chroma db\")\n# creating the vector store\n# vector_store = Chroma.from_documents(\n#     chunks, embedding_model, persist_directory=\"db\"\n# )\n# Load the existing Chroma vector store\n# vector_store = Chroma(persist_directory=\"db\", embedding_function=embedding_model.embed_query)\nvector_store = Chroma(persist_directory=\"db\", embedding_function=embedding_model)\nprint(\"Chroma DB loaded successfully\")",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "vector_store",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "vector_store = Chroma(persist_directory=\"db\", embedding_function=embedding_model)\nprint(\"Chroma DB loaded successfully\")\ndef handle_query(query, context=\"\"):\n    # If document search is triggered\n    if \"section\" in query.lower() or \"pdf\" in query.lower():\n        results = vector_store.similarity_search(query)\n        if results:\n            context = results[0].page_content  # Add the first result to the context\n    # Construct the full prompt with context and query\n    full_prompt = chat_prompt.format(query=query, context=context)",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "chat_history",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "chat_history = []\nwhile True:\n    user_query = input(\"Ask your question: \")\n    if user_query.lower() == \"exit\":\n        break\n    # Add previous conversation to context\n    context = \" \".join(chat_history)\n    # Handle the query\n    response = handle_query(user_query, context)\n    # Update chat history",
        "detail": "app",
        "documentation": {}
    }
]